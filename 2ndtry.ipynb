{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             REQUEST            INTENT  label\n",
      "0  Good day! Are there any single phase equivalen...  CatalogSelection      0\n",
      "1  Good day! I would like to ask for the differen...  CatalogSelection      0\n",
      "2               hi do we have a current transformer?  CatalogSelection      0\n",
      "3  Hi, may I ask if these items are still active,...  CatalogSelection      0\n",
      "4  I am looking for a magnetic contactor for LC1K...  CatalogSelection      0\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('Intent_detection_Data(Hoja1).csv', encoding='latin1')\n",
    "\n",
    "# Map intents to numerical IDs\n",
    "label2id = {label: idx for idx, label in enumerate(df[\"INTENT\"].unique())}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "df[\"label\"] = df[\"INTENT\"].map(label2id)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df[\"REQUEST\"], df[\"label\"], test_size=0.2, stratify=df[\"INTENT\"], random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/kawtarissam/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /Users/kawtarissam/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n",
      "loading file tokenizer.json from cache at /Users/kawtarissam/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/kawtarissam/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/kawtarissam/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /Users/kawtarissam/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/kawtarissam/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n",
      "A pretrained model of type `BertForSequenceClassification` contains parameters that have been renamed internally (a few are listed below but more are present in the model):\n",
      "* `bert.embeddings.LayerNorm.gamma` -> `bert.embeddings.LayerNorm.weight`\n",
      "* `bert.encoder.layer.0.attention.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.0.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.1.attention.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.1.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.10.attention.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.10.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.11.attention.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.11.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.2.attention.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.2.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.3.attention.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.3.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.4.attention.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.4.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.5.attention.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.5.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.6.attention.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.6.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.7.attention.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.7.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.8.attention.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.8.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.9.attention.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.encoder.layer.9.output.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `cls.predictions.transform.LayerNorm.gamma` -> `{'bert.embeddings.LayerNorm.gamma': 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.0.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.1.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.10.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.11.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.2.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.3.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.4.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.5.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.6.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.7.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.8.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.gamma': {...}, 'bert.encoder.layer.9.output.LayerNorm.gamma': {...}, 'cls.predictions.transform.LayerNorm.gamma': {...}}`\n",
      "* `bert.embeddings.LayerNorm.beta` -> `bert.embeddings.LayerNorm.bias`\n",
      "* `bert.encoder.layer.0.attention.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.0.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.1.attention.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.1.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.10.attention.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.10.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.11.attention.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.11.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.2.attention.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.2.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.3.attention.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.3.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.4.attention.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.4.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.5.attention.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.5.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.6.attention.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.6.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.7.attention.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.7.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.8.attention.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.8.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.9.attention.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `bert.encoder.layer.9.output.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "* `cls.predictions.transform.LayerNorm.beta` -> `{'bert.embeddings.LayerNorm.beta': 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.0.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.1.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.10.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.11.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.2.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.3.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.4.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.5.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.6.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.7.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.8.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.attention.output.LayerNorm.beta': {...}, 'bert.encoder.layer.9.output.LayerNorm.beta': {...}, 'cls.predictions.transform.LayerNorm.beta': {...}}`\n",
      "If you are using a model from the Hub, consider submitting a PR to adjust these weights and help future users.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels=len(label2id)\n",
    ")\n",
    "\n",
    "# Handle padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets from training and validation data\n",
    "train_dataset = Dataset.from_dict({\"REQUEST\": train_texts.tolist(), \"label\": train_labels.tolist()})\n",
    "val_dataset = Dataset.from_dict({\"REQUEST\": val_texts.tolist(), \"label\": val_labels.tolist()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 128/128 [00:00<00:00, 2474.95 examples/s]\n",
      "Map: 100%|██████████| 32/32 [00:00<00:00, 3751.09 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Define tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"REQUEST\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Apply tokenization to train and validation datasets\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Format datasets for PyTorch\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 30522. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 768, padding_idx=0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjust the model's token embeddings to match the tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kawtarissam/Library/Python/3.9/lib/python/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/Users/kawtarissam/Library/Python/3.9/lib/python/site-packages/transformers/training_args.py:1583: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/Users/kawtarissam/Desktop/chatbot2/my_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,  # Adjust as needed\n",
    "    log_level=\"info\",\n",
    "    no_cuda=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "# Function to compute metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data collator for dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: REQUEST. If REQUEST are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 128\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 640\n",
      "  Number of trainable parameters = 109,485,316\n",
      " 16%|█▌        | 100/640 [03:49<20:37,  2.29s/it]\n",
      " 16%|█▌        | 100/640 [03:49<20:37,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4173, 'grad_norm': 21.55948257446289, 'learning_rate': 1.6875e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 128/640 [04:52<19:58,  2.34s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: REQUEST. If REQUEST are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 32\n",
      "  Batch size = 1\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                 \n",
      "\n",
      " 20%|██        | 128/640 [05:06<19:58,  2.34s/it]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1689668893814087, 'eval_accuracy': 0.46875, 'eval_runtime': 13.664, 'eval_samples_per_second': 2.342, 'eval_steps_per_second': 2.342, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 200/640 [07:50<16:55,  2.31s/it]\n",
      " 31%|███▏      | 200/640 [07:50<16:55,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0435, 'grad_norm': 10.0105562210083, 'learning_rate': 1.375e-05, 'epoch': 1.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 256/640 [10:01<14:14,  2.22s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: REQUEST. If REQUEST are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 32\n",
      "  Batch size = 1\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                 \n",
      "\n",
      " 40%|████      | 256/640 [10:14<14:14,  2.22s/it]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.524774432182312, 'eval_accuracy': 0.84375, 'eval_runtime': 13.1861, 'eval_samples_per_second': 2.427, 'eval_steps_per_second': 2.427, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 300/640 [11:53<12:19,  2.18s/it]\n",
      " 47%|████▋     | 300/640 [11:53<12:19,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4787, 'grad_norm': 41.179847717285156, 'learning_rate': 1.0625e-05, 'epoch': 2.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 384/640 [14:59<09:04,  2.13s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: REQUEST. If REQUEST are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 32\n",
      "  Batch size = 1\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                 \n",
      "\n",
      " 60%|██████    | 384/640 [15:13<09:04,  2.13s/it]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.40331292152404785, 'eval_accuracy': 0.84375, 'eval_runtime': 14.6067, 'eval_samples_per_second': 2.191, 'eval_steps_per_second': 2.191, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 400/640 [15:50<08:52,  2.22s/it]\n",
      " 62%|██████▎   | 400/640 [15:50<08:52,  2.22s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1821, 'grad_norm': 0.5565797090530396, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 500/640 [19:46<05:24,  2.32s/it]\n",
      " 78%|███████▊  | 500/640 [19:46<05:24,  2.32s/it]  Saving model checkpoint to /Users/kawtarissam/Desktop/chatbot2/my_model/checkpoint-500\n",
      "Configuration saved in /Users/kawtarissam/Desktop/chatbot2/my_model/checkpoint-500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1815, 'grad_norm': 0.2898178994655609, 'learning_rate': 4.3750000000000005e-06, 'epoch': 3.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in /Users/kawtarissam/Desktop/chatbot2/my_model/checkpoint-500/model.safetensors\n",
      " 80%|████████  | 512/640 [20:26<06:28,  3.04s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: REQUEST. If REQUEST are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 32\n",
      "  Batch size = 1\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                   \n",
      "\n",
      " 80%|████████  | 512/640 [20:42<06:28,  3.04s/it]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.35099202394485474, 'eval_accuracy': 0.9375, 'eval_runtime': 15.1224, 'eval_samples_per_second': 2.116, 'eval_steps_per_second': 2.116, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 600/640 [24:07<01:22,  2.07s/it]\n",
      " 94%|█████████▍| 600/640 [24:07<01:22,  2.07s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1539, 'grad_norm': 0.26460182666778564, 'learning_rate': 1.25e-06, 'epoch': 4.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 640/640 [25:29<00:00,  2.04s/it]Saving model checkpoint to /Users/kawtarissam/Desktop/chatbot2/my_model/checkpoint-640\n",
      "Configuration saved in /Users/kawtarissam/Desktop/chatbot2/my_model/checkpoint-640/config.json\n",
      "Model weights saved in /Users/kawtarissam/Desktop/chatbot2/my_model/checkpoint-640/model.safetensors\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: REQUEST. If REQUEST are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 32\n",
      "  Batch size = 1\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                   \n",
      "\n",
      "100%|██████████| 640/640 [25:46<00:00,  2.04s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 640/640 [25:46<00:00,  2.42s/it]  \n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: REQUEST. If REQUEST are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 32\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3701702952384949, 'eval_accuracy': 0.90625, 'eval_runtime': 15.4343, 'eval_samples_per_second': 2.073, 'eval_steps_per_second': 2.073, 'epoch': 5.0}\n",
      "{'train_runtime': 1546.8914, 'train_samples_per_second': 0.414, 'train_steps_per_second': 0.414, 'train_loss': 0.5407536635175347, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:14<00:00,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.90625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "results = trainer.evaluate()\n",
    "print(\"Validation Accuracy:\", results[\"eval_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.90625\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation Accuracy:\", results[\"eval_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./my_model/config.json\n",
      "Model weights saved in ./my_model/model.safetensors\n",
      "tokenizer config file saved in ./my_model/tokenizer_config.json\n",
      "Special tokens file saved in ./my_model/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./my_model/tokenizer_config.json',\n",
       " './my_model/special_tokens_map.json',\n",
       " './my_model/vocab.txt',\n",
       " './my_model/added_tokens.json',\n",
       " './my_model/tokenizer.json')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./my_model\")  # Saving the model\n",
    "tokenizer.save_pretrained(\"./my_model\")  #  tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
